{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting to explore GANs using Keras/Tensorflow backed\n",
    "## Motivation\n",
    "I got the inspiration to explore that domain because I really like japanese anime and nowadays, there's tools like AniGAN or AnimeGANV2\n",
    "that transform a picture of a real humain into his anime counterpart. Fascinating!\n",
    "## Goal\n",
    "Make a simple DCGAN (Deep Convolutionnal Generative Adversarial Network) that will learn how to transform a picture into an anime version.\n",
    "## Required knowledge\n",
    "A GAN model consist of 2 different model :\n",
    "\n",
    "1.  The Discriminator. This model will get an image as input and will output the probability of that image being \"fake\". For the above mentionned goal, it will be the probability that the image is a \"anime\" image.\n",
    "2.  The Generator. This model will take as input an image (whatever it is) and will try to transform it into a good enough fake that the            Discriminator will think it's not fake. For the above mentionned goal, it will generate an image that will have an \"anime\" style good enough that the Discriminator will think it's real.\n",
    "\n",
    "To train a DCGAN, we will give the Generator noises as input and with that, it will need to learn how to fool the Discriminator.\n",
    "\n",
    "The hypothesis here is : if the Generator can transform a noise into something close to being \"anime\", then it will \"learn\" what needs to be transformed to be considered an anime version.\n",
    "## Steps\n",
    "### 0. Initiate with library and data imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import multiprocessing as mp\n",
    " \n",
    "# Try to import images and read pixel by pixel\n",
    "dataset = \"general\"\n",
    "process_number = 10\n",
    "\n",
    "dataset_path = \"%s\\\\dataset\\\\%s\\\\\" % (os.getcwd(), dataset)\n",
    "\n",
    "filenames = next(os.walk(dataset_path), (None, None, []))[2]\n",
    "file_per_batch = len(filenames) // process_number\n",
    "\n",
    "images = []\n",
    "def image_load(image_path):\n",
    "    img = Image.open(dataset_path + image_path).resize((128,128), Image.LANCZOS)\n",
    "    img = img.convert(\"RGB\")\n",
    "    img.save(dataset_path + image_path, quality=95)\n",
    "    pixels = np.array(img, dtype=np.uint8)\n",
    "    images.append(pixels)\n",
    "\n",
    "for filename in filenames:\n",
    "    image_load(filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create the Discriminator model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "010d72da20a09e5bca4b1cca71edb635b8a8a147b32fbccaea84cbf59b145b1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
